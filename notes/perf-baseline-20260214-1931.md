# Perf Baseline - 2026-02-14 19:31 EST

Goal: quick throughput reality check to find obvious improvement targets (not final tuning).

## Environment

- Test window: 2026-02-14 19:31 EST
- Hosts tested: `rb1-pve` (`192.168.5.98`), `rb2` (`192.168.5.108`), `mba` (`192.168.5.66`)
- Tooling: `iperf3` installed on all three Proxmox hosts during this run.
- Raw outputs:
  - `notes/iperf3-matrix-20260214-193115.log`
  - `notes/iperf3-matrix-20260214-193115-summary.txt`

## Link State (Before/After)

- `rb1` `enx90203a1be8d6`: `up`, `1000/full`, counters stable
- `rb2` `enx00051bde7e6e`: `up`, `1000/full`, counters stable
- `mba` `nic0`: `up`, `1000/full`, counters stable

No meaningful NIC error/drop counter growth during the short test set.

## Throughput Highlights

### `rb1 <-> rb2`

- `~868-878 Mbps` across `P1`, `P4`, and `P4 -R`
- retransmits: `0`
- interpretation: healthy 1GbE path operating near expected ceiling

### `rb1 <-> mba`

- `~278-297 Mbps`
- retransmits: `~5k-6.3k` per 12-second run
- interpretation: major bottleneck on MBA path

### `rb2 <-> mba`

- `~315-317 Mbps`
- retransmits: `~8.2k-8.3k` per 12-second run
- interpretation: same MBA-centric bottleneck

## Root-Cause Signal

`lsusb -t` shows MBA Ethernet adapter (`r8152`) on a `480M` bus path:

- `mba`: `r8152 ... 480M`

This strongly matches observed MBA ceiling (~300 Mbps class). By contrast:

- `rb2` `r8152`: `5000M`
- `rb1` `ax88179_178a`: `5000M`

## Improvement Priorities

1. Do not treat MBA as a performance node; keep it as continuity/quorum/fallback.
2. Prioritize throughput work between `rb1`, `rb2`, and workstation/mac mini only.
3. For speed gains, move heavy traffic to 2.5Gb-capable NIC paths and the fast switch.
4. If MBA speed matters later, use a higher-bandwidth adapter path (if physically supported), but ROI is likely low for this host class.

## Next Fast Checks

1. Add workstation/mac mini to test matrix (install `iperf3` there).
2. Re-run matrix after any NIC/switch/cable change and compare to this baseline.
